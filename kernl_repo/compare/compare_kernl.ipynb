{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizer in /usr/local/lib/python3.9/dist-packages (3.4.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mWed Jan 18 17:12:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76.02    Driver Version: 517.48       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   33C    P8    26W / 350W |    796MiB / 24576MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! pip install tokenizer sentencepiece\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import time\n",
    "import torch._dynamo as torchdynamo\n",
    "import torch\n",
    "from kernl.model_optimization import optimize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# default cache size needs to be increased to store the many graphs with generative models\n",
    "torchdynamo.config.cache_size_limit = 512\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"translate English to French: The house in the woods is wonderful, can we buy it ?\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length', truncation=True, max_length=20\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "0.42411463260650634 s / inference\n"
     ]
    }
   ],
   "source": [
    "# vanilla\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=22,\n",
    "        max_length=22,\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "end = time.time()\n",
    "print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "0.33517866134643554 s / inference\n"
     ]
    }
   ],
   "source": [
    "# vanilla inference mode\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/_dynamo/eval_frame.py:372: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled.Consider setting `torch.set_float32_matmul_precision('high')`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# dynamo optimize\n",
    "model.generate2 = torchdynamo.optimize(\"inductor\")(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 271, in call_method\n",
      "    return getattr(self_obj, target)(*args_tail, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %view : [#users=1] = call_method[target=view](args = (%inputs_tensor, -1, 20), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_encoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 932, in forward\n",
      "    input_ids = input_ids.view(-1, input_shape[-1])\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 601, in _prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
      "\n",
      "[2023-01-18 17:03:37,405] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:03:46,214] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,241] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,263] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,286] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,311] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,335] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,360] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,385] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,410] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,434] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,455] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,477] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:46,920] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:03:52,773] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:52,817] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:52,853] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:52,888] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:52,922] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:52,958] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:52,993] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:53,028] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:53,063] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:53,099] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:53,134] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:53,170] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:53,579] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:03:59,070] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,116] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,165] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,240] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,278] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,315] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,352] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,389] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,426] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,464] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,502] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,538] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:03:59,975] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:05,565] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,614] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,652] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,689] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,727] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,765] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,803] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,840] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,878] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,915] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,953] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:05,991] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:06,463] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:12,172] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,216] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,253] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,292] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,331] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,367] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,405] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,445] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,485] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,522] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,558] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:12,596] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:13,079] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:18,676] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,734] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,775] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,814] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,852] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,891] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,928] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:18,966] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:19,004] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:19,053] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:19,112] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:19,150] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:19,630] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:25,380] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,431] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,469] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,509] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,547] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,587] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,625] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,673] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,712] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,779] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,833] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:25,873] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:26,370] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:32,161] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,202] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,235] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,269] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,306] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,340] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,380] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,416] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,455] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,493] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,531] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:32,567] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:33,099] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:38,900] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:38,940] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:38,975] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,010] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,044] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,079] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,113] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,148] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,183] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,217] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,251] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,285] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:39,756] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:45,154] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,195] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,228] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,261] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,295] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,332] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,364] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,397] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,430] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,465] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,498] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:45,531] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:46,033] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:51,428] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,470] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,503] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,536] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,570] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,604] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,637] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,672] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,705] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,738] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,771] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:51,805] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:52,346] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:04:57,720] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,762] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,796] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,830] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,864] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,897] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,936] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:57,970] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:58,004] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:58,038] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:58,071] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:58,107] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:04:58,634] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:03,924] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:03,968] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,001] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,034] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,068] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,102] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,135] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,169] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,204] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,238] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,271] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,305] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:04,828] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:10,315] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,358] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,419] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,456] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,491] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,524] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,559] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,594] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,629] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,663] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,696] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:10,729] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:11,273] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:16,800] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:16,841] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:16,874] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:16,908] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:16,941] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:16,976] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,010] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,045] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,079] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,112] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,162] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,195] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:17,776] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:23,224] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,269] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,303] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,338] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,373] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,407] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,439] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,471] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,506] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,539] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,573] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:23,611] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:24,229] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:29,654] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,691] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,745] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,779] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,823] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,861] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,895] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,926] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,961] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:29,995] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:30,027] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:30,061] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:30,712] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:36,152] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,194] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,230] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,264] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,300] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,339] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,378] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,414] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,448] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,482] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,515] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:36,551] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:37,164] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:42,597] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,639] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,673] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,708] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,745] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,779] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,811] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,845] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,879] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,914] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,946] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:42,979] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:43,576] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:49,037] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,079] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,113] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,146] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,180] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,214] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,247] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,281] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,314] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,348] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,381] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:49,414] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:50,055] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:05:55,638] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,682] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,717] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,753] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,787] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,823] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,856] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,888] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,925] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,964] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:55,999] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:56,032] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:05:56,708] torch._inductor.compile_fx: [WARNING] skipping cudagraphs due to input mutation\n"
     ]
    }
   ],
   "source": [
    "# dynamo warm up\n",
    "with torch.inference_mode():\n",
    "    output = model.generate2(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=22,\n",
    "        max_length=22,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "0.09746694564819336 s / inference\n"
     ]
    }
   ],
   "source": [
    "# dynamo inference mode\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate2(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "C'est une belle journe.........\n",
      "0.09948692321777344 s / inference\n"
     ]
    }
   ],
   "source": [
    "# dynamo inference mode on new input (Note: this does not trigger dynamo to recompile, and output is different and makes sense)\n",
    "\n",
    "new_input_ids = tokenizer(\n",
    "    \"translate English to French: It is a nice day.\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length', truncation=True, max_length=20\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate2(\n",
    "            inputs=new_input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "0.05882694721221924 s / inference\n"
     ]
    }
   ],
   "source": [
    "# dynamo inference mode on new input without output length constraint (Note: this does not trigger dynamo to recompile)\n",
    "\n",
    "new_input_ids = tokenizer(\n",
    "    \"translate English to French: It is a nice day.\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length', truncation=True, max_length=20\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate2(\n",
    "            inputs=new_input_ids[\"input_ids\"]\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "[2023-01-18 17:07:35,600] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:36,183] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:36,611] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:37,024] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:37,457] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:37,869] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:38,277] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:38,690] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un beau jour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cest une journe agrable.\n",
      "C'est une belle journe.\n",
      "Cest une belle journe.\n",
      "Cest un beau jour.\n",
      "C'est une belle journe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-18 17:07:39,582] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n",
      "[2023-01-18 17:07:39,974] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.topk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "Cette journe m'est agrable.\n",
      "0.7632161617279053 s / inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# dynamo inference mode on new input without output length constraint with sampling (Note: this does not trigger dynamo to recompile)\n",
    "\n",
    "new_input_ids = tokenizer(\n",
    "    \"translate English to French: It is a nice day.\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length', truncation=True, max_length=20\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate2(\n",
    "            inputs=new_input_ids[\"input_ids\"],\n",
    "            do_sample=True\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:08:43,714] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,758] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,792] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,827] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,859] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,895] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,930] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,962] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:43,995] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:44,027] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:44,057] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:44,089] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:08:52,295] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,344] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,384] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,429] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,472] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,513] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,554] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,593] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,635] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,677] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,724] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:08:52,765] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:09:07,730] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:07,782] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:07,826] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:07,871] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:07,913] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:07,956] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:07,997] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:08,038] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:08,080] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:08,121] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:08,161] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:08,221] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:09:19,127] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,180] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,221] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,262] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,304] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,346] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,389] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,451] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,504] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,549] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,590] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:19,631] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:09:30,976] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,030] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,071] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,115] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,156] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,200] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,240] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,280] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,327] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,376] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,420] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:31,461] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:09:47,455] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,510] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,552] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,600] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,647] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,691] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,732] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,773] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,816] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,858] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,900] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:09:47,942] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:10:03,855] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:03,909] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:03,953] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:03,997] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,045] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,088] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,130] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,173] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,215] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,259] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,300] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:04,342] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:10:18,513] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,559] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,599] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,636] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,676] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,713] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,750] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,788] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,826] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,863] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,926] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:18,967] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 1273, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out, _num_aliasing_metadata_outs = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 289, in inner\n",
      "    outs = f(*f_args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_functorch/aot_autograd.py\", line 2327, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/fx/interpreter.py\", line 294, in call_module\n",
      "    return submod(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1482, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %self_decoder_block_0_layer_0_self_attention_q : [#users=1] = call_module[target=self_decoder_block_0_layer_0_SelfAttention_q](args = (%mul_4,), kwargs = {})\n",
      "Original traceback:\n",
      "Module stack: {'self_decoder': \"<class 'transformers.models.t5.modeling_t5.T5Stack'>\", 'self_decoder_block_0': \"<class 'transformers.models.t5.modeling_t5.T5Block'>\", 'sub0_0': \"<class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>\", 'self_decoder_block_0_layer_0_SelfAttention': \"<class 'transformers.models.t5.modeling_t5.T5Attention'>\", 'self_decoder_block_0_layer_0_SelfAttention_q': \"<class 'torch.nn.modules.linear.Linear'>\"}\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 498, in forward\n",
      "    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      " |   File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\", line 1648, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions\n",
      "  warnings.warn(\"functional_call was passed multiple values for tied weights. \"\n",
      "[2023-01-18 17:10:35,670] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,718] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,756] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,797] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,836] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,878] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,916] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,955] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:35,994] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:36,034] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:36,072] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n",
      "[2023-01-18 17:10:36,111] torch._inductor.optimize_indexing: [WARNING] unhandled ValueRange op name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journe.\n",
      "C'est une belle journe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "C'est une belle journe.\n",
      "13.003740215301514 s / inference\n"
     ]
    }
   ],
   "source": [
    "# dynamo inference mode on new input without output length constraint with beam-search (Note: this triggers dynamo to recompile, resulting in much longer infernce time due to dynamo warm-up)\n",
    "\n",
    "new_input_ids = tokenizer(\n",
    "    \"translate English to French: It is a nice day.\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length', truncation=True, max_length=20\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate2(\n",
    "            inputs=new_input_ids[\"input_ids\"],\n",
    "            num_beams=3\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernl (Note: need to restart notebook and run since running dynamo first then run this would cause an error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernl optimize\n",
    "optimize_model(model.encoder)\n",
    "optimize_model(model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.9240474000003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kernl warmup with fp16\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.perf_counter()\n",
    "    output = model.generate(inputs=input_ids[\"input_ids\"], min_length=22, max_length=22)\n",
    "    print(time.perf_counter() - start)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "0.1228116512298584 s / inference\n"
     ]
    }
   ],
   "source": [
    "# kernl inference model fp16\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "        print(output[0])\n",
    "end = time.time()\n",
    "print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "0.12446894645690917 s / inference\n"
     ]
    }
   ],
   "source": [
    "# kernl inference model fp16 on new input\n",
    "\n",
    "new_input_ids = tokenizer(\n",
    "    \"translate English to French: It is a nice day.\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length', truncation=True, max_length=20\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = model.generate(\n",
    "            inputs=new_input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "        print(output[0])\n",
    "end = time.time()\n",
    "print(f'{(end - start) / 10} s / inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
